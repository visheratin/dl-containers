{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from barbar import Bar\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=os.getenv('TENSORBOARD_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'name': 'DistilBERT no training',\n",
    "    'n_epochs': 5,\n",
    "    'batch_size': 6,\n",
    "    'val_split': 0.15,\n",
    "    'device': 'cuda',\n",
    "    'data_dir': '/jupyter/data/news/',\n",
    "    'learning_rate': 5e-5,\n",
    "    'margin': 3.0,\n",
    "    'class_size': 200,\n",
    "    'item_triplets': 50\n",
    "}\n",
    "\n",
    "def filter_parameters(parameters):\n",
    "    res = dict(parameters)\n",
    "    del res['data_dir']\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%d-%m-%Y %H-%M\")\n",
    "    res['start_time'] = date_time\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(row, tkn: transformers.AutoTokenizer):\n",
    "    maxlen = 40\n",
    "    title_tokens = tkn.encode_plus(\n",
    "        row['title'],\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    t = torch.cat((title_tokens['input_ids'], title_tokens['attention_mask']), dim=0)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of extract_tokens with abstracts\n",
    "def extract_tokens_abstracts(row, tkn: transformers.AutoTokenizer):\n",
    "    maxlen = 40\n",
    "    title_tokens = tkn.encode_plus(\n",
    "        row['title'],\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=maxlen,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    if type(row['abs']) is float:\n",
    "        abstract_tokens = title_tokens\n",
    "    else:\n",
    "        abstract_tokens = tkn.encode_plus(\n",
    "            row['abs'],\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=maxlen,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "    t = torch.cat((title_tokens['input_ids'], title_tokens['attention_mask']), dim=0)\n",
    "    a = torch.cat((abstract_tokens['input_ids'], abstract_tokens['attention_mask']), dim=0)\n",
    "    r = torch.cat((t, a))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets(news_df: pd.DataFrame, tkn: transformers.AutoTokenizer, \n",
    "    limit: int, conn_num: int):\n",
    "    random.seed(42)\n",
    "    cats_dict = {}\n",
    "    for _, row in news_df.iterrows():\n",
    "        subcat = row['cats']\n",
    "        if subcat in cats_dict:\n",
    "            num = len(cats_dict[subcat])\n",
    "            if num == limit:\n",
    "                continue\n",
    "            cats_dict[subcat].append(extract_tokens(row, tkn))\n",
    "        else:\n",
    "            cats_dict[subcat] = [extract_tokens(row, tkn)]\n",
    "    \n",
    "    categories = list(cats_dict.keys())\n",
    "    \n",
    "    res = []\n",
    "    hits = []\n",
    "    for class_idx in categories:\n",
    "        cat_items = cats_dict[class_idx]\n",
    "        for n_idx in range(len(cat_items)):\n",
    "            if len(cat_items)-1 <= conn_num:\n",
    "                for pos_idx in range(len(cat_items)):\n",
    "                    if pos_idx == n_idx:\n",
    "                        continue\n",
    "                    # select negative sample index from other subcategories\n",
    "                    neg_class_idx = random.randrange(0, len(categories))\n",
    "                    if categories[neg_class_idx] == class_idx:\n",
    "                        continue\n",
    "                    neg_items = cats_dict[categories[neg_class_idx]]\n",
    "                    neg_idx = random.randrange(0, len(neg_items))\n",
    "                    # check that this combination was not selected before\n",
    "                    comb = ((n_idx, class_idx), (pos_idx, class_idx), (neg_idx, categories[neg_class_idx]))\n",
    "                    if comb in hits:\n",
    "                        continue\n",
    "                    item = (cat_items[n_idx], neg_items[neg_idx], cat_items[pos_idx])\n",
    "                    res.append(item)\n",
    "                    hits.append(comb)\n",
    "                continue\n",
    "            c = 0\n",
    "            while True:\n",
    "                if c == conn_num:\n",
    "                    break\n",
    "                # select negative sample index from other subcategories\n",
    "                neg_class_idx = random.randrange(0, len(categories))\n",
    "                if categories[neg_class_idx] == class_idx:\n",
    "                    continue\n",
    "                neg_items = cats_dict[categories[neg_class_idx]]\n",
    "                neg_idx = random.randrange(0, len(neg_items))\n",
    "                # select positive sample index from current subcategory\n",
    "                pos_idx = random.randrange(0, len(cat_items))\n",
    "                if pos_idx == n_idx:\n",
    "                    continue\n",
    "                # check that this combination was not selected before\n",
    "                comb = ((n_idx, class_idx), (pos_idx, class_idx), (neg_idx, categories[neg_class_idx]))\n",
    "                if comb in hits:\n",
    "                    continue\n",
    "                item = (cat_items[n_idx], neg_items[neg_idx], cat_items[pos_idx])\n",
    "                res.append(item)\n",
    "                hits.append(comb)\n",
    "                c += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, path, class_size, item_triplets):\n",
    "        news_df = pd.read_csv(path, sep='\\t', header=None,\n",
    "                     names=['id','cats','subcat','title','abs','link','title-ent','abs-ent'])\n",
    "        tkn = AutoTokenizer.from_pretrained(\"/jupyter/models/distilbert/\")\n",
    "        self.pairs = extract_triplets(news_df, tkn, class_size, item_triplets)\n",
    "        self.df = news_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_ds = NewsDataset(parameters['data_dir']+'news.tsv', parameters['class_size'], parameters['item_triplets'])\n",
    "parameters['ds_size'] = len(pairs_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_size = len(pairs_ds.pairs)\n",
    "indices = list(range(ds_size))\n",
    "split = int(np.floor(parameters['val_split'] * ds_size))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_len = len(train_indices) - len(train_indices)%parameters['batch_size']\n",
    "train_indices = train_indices[:train_len]\n",
    "val_len = len(val_indices) - len(val_indices)%parameters['batch_size']\n",
    "val_indices = val_indices[:val_len]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(pairs_ds, batch_size=parameters['batch_size'], sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(pairs_ds, batch_size=parameters['batch_size'], sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.tf_layer = AutoModel.from_pretrained('/jupyter/models/distilbert', torchscript=True)\n",
    "        # uncomment these lines for freezing DistilBERT weights\n",
    "        # for p in self.tf_layer.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.bert_proc = nn.Sequential(\n",
    "            nn.Linear(768, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        bert_title = self.tf_layer(tokens[:,0], tokens[:,1])\n",
    "        out = self.bert_proc(bert_title[0][:,0,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of SiamseseNet with abstracts\n",
    "class SiameseNetAbstracts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.tf_layer = AutoModel.from_pretrained('/jupyter/models/distilbert', torchscript=True)\n",
    "        # uncomment these lines for freezing DistilBERT weights\n",
    "        # for p in self.tf_layer.parameters():\n",
    "        #     p.requires_grad = False\n",
    "        self.bert_proc = nn.Sequential(\n",
    "            nn.Linear(1536, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        bert_title = self.tf_layer(tokens[:,0], tokens[:,1])\n",
    "        bert_abs = self.tf_layer(tokens[:,2], tokens[:,3])\n",
    "        concat = torch.cat((bert_title[0][:,0,:], bert_abs[0][:,0,:]), dim=-1)\n",
    "        out = self.bert_proc(concat)\n",
    "        out = self.bert_proc(bert_title[0][:,0,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSiamese():\n",
    "    def __init__(self, device, report_step):\n",
    "        self.model = SiameseNet()\n",
    "        self.model = self.model.to(device)\n",
    "        self.device = device\n",
    "        self.report_step = report_step\n",
    "        self.run_counter = 1\n",
    "        self.train_counter = 1\n",
    "        self.val_counter = 1\n",
    "        \n",
    "    def setup(self, crit, opt, writer):\n",
    "        self.crit = crit\n",
    "        self.opt = opt\n",
    "        self.writer = writer\n",
    "    \n",
    "    def train_step(self, loader):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        counter = 0\n",
    "        total_loss = 0.0\n",
    "        total_counter = 0\n",
    "        for idx, triplets in enumerate(loader):\n",
    "            self.opt.zero_grad()\n",
    "            anchor, negative, positive = triplets\n",
    "            anchor = anchor.to(self.device)\n",
    "            negative = negative.to(self.device)\n",
    "            positive = positive.to(self.device)\n",
    "            anchor_out = self.model(anchor)\n",
    "            negative_out = self.model(negative)\n",
    "            positive_out = self.model(positive)\n",
    "            loss = self.crit(anchor_out, negative_out, positive_out)\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            running_loss += loss.item()\n",
    "            counter += 1\n",
    "            total_loss += loss.item()\n",
    "            total_counter += 1\n",
    "            if idx % self.report_step == 0:\n",
    "                self.writer.add_scalar(\"Train/Running loss\", running_loss/counter, self.run_counter)\n",
    "                self.writer.flush()\n",
    "                self.run_counter += 1\n",
    "                running_loss = 0.0\n",
    "                counter = 0\n",
    "        avg_loss = total_loss/total_counter\n",
    "        self.writer.add_scalar(\"Train/Total loss\", avg_loss, self.train_counter)\n",
    "        self.train_counter += 1\n",
    "        return total_loss\n",
    "                \n",
    "    def val_step(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        counter = 0\n",
    "        for idx, triplets in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                anchor, negative, positive = triplets\n",
    "                anchor = anchor.to(self.device)\n",
    "                negative = negative.to(self.device)\n",
    "                positive = positive.to(self.device)\n",
    "                anchor_out = self.model(anchor)\n",
    "                negative_out = self.model(negative)\n",
    "                positive_out = self.model(positive)\n",
    "                loss = self.crit(anchor_out, negative_out, positive_out)\n",
    "                total_loss += loss.item()\n",
    "                counter += 1\n",
    "        avg_loss = total_loss/counter\n",
    "        self.writer.add_scalar(\"Validation/Loss\", avg_loss, self.val_counter)\n",
    "        self.writer.flush()\n",
    "        self.val_counter += 1\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class TripletLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, negative, positive):\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, keepdim = True)\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, keepdim = True)\n",
    "        loss = torch.mean(torch.clamp(pos_dist - neg_dist + self.margin, min=0.0))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev = torch.device(parameters['device'])\n",
    "net = NewsSiamese(dev, 10)\n",
    "crit = TripletLoss(margin=parameters['margin'])\n",
    "opt = optim.AdamW(net.model.parameters(), lr=parameters['learning_rate'])\n",
    "writer = SummaryWriter('/jupyter/runs/{}'.format(parameters['name']))\n",
    "net.setup(crit, opt, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in range(parameters['n_epochs']):\n",
    "    print('\\nEpoch {}'.format(n+1))\n",
    "    print('Training...')\n",
    "    train_loss = net.train_step(Bar(train_loader))\n",
    "    print('\\nValidating...')\n",
    "    val_loss = net.val_step(Bar(validation_loader))\n",
    "    net.writer.add_hparams(filter_parameters(parameters), \n",
    "        {'hparams/train_loss': train_loss, 'hparams/validation_loss': val_loss}, 'parameters', n+1)\n",
    "    net.writer.flush()\n",
    "    print('\\n---------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}